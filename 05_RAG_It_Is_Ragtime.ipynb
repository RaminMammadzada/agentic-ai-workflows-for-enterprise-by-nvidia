{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a RAG Workflow with AgentIQ\n",
    "\n",
    "## Introduction to RAG\n",
    "\n",
    "**Retrieval Augmented Generation (RAG)** is a powerful technique that enhances Large Language Models (LLMs) by providing them with relevant information retrieved from external knowledge sources. RAG combines the strengths of retrieval-based systems with the generative capabilities of LLMs to produce more accurate, up-to-date, and contextually relevant responses.\n",
    "\n",
    "### How RAG Works\n",
    "\n",
    "RAG operates in three main steps:\n",
    "\n",
    "1. **Retrieval**: When a user asks a question, the system searches through a knowledge base to find relevant documents or passages.\n",
    "2. **Augmentation**: The retrieved information is added to the prompt sent to the LLM.\n",
    "3. **Generation**: The LLM generates a response based on both its pre-trained knowledge and the retrieved information.\n",
    "\n",
    "### Benefits of RAG\n",
    "\n",
    "- **Reduced Hallucinations**: By grounding responses in retrieved facts, RAG helps minimize the LLM's tendency to generate plausible but incorrect information.\n",
    "- **Up-to-date Knowledge**: RAG can access information that wasn't available during the LLM's training.\n",
    "- **Domain Adaptation**: RAG allows LLMs to work with specialized knowledge without fine-tuning.\n",
    "- **Transparency**: The retrieved documents provide a source for the information, making the system more explainable.\n",
    "\n",
    "## RAG in AgentIQ\n",
    "\n",
    "AgentIQ provides robust support for building RAG workflows through integration with popular frameworks like LlamaIndex and LangChain. In this notebook, we'll build a RAG workflow using the `book_knowledge_rag` tool, which allows us to:\n",
    "\n",
    "1. Ingest and process documents\n",
    "2. Create vector embeddings for efficient retrieval\n",
    "3. Query the knowledge base with natural language\n",
    "4. Evaluate and profile the RAG system's performance\n",
    "\n",
    "By the end of this notebook, you'll understand how to:\n",
    "- Configure a RAG workflow in AgentIQ\n",
    "- Evaluate its performance using standard metrics\n",
    "- Profile and optimize your RAG system\n",
    "- Enhance performance by upgrading components like the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Our Environment\n",
    "\n",
    "Let's start by setting up our environment for the RAG workflow. We'll need to set the NVIDIA API key and create our project structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Opening Phoenix (optional)\n",
    "\n",
    "If you want to observe detailed data on the RAG calls, you can open Phoenix as these workflow configs have been instrumented for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "const href = window.location.hostname;\n",
    "let a = document.createElement('a');\n",
    "let link = document.createTextNode('Click here to open Phoenix!');\n",
    "a.appendChild(link);\n",
    "a.href = \"http://\" + href + \"/phoenix\";\n",
    "a.style.color = \"navy\"\n",
    "a.target = \"_blank\"\n",
    "element.append(a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating the AgentIQ workflow\n",
    "\n",
    "Let's create a workflows directory (if it doesn't exist from previous notebooks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the AgentIQ cli to create the new workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aiq workflow create --no-install --workflow-dir workflows ragtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that the project is properly in place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree workflows/ragtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see the following directory structure for our RAG project. We'll follow this standard structure:\n",
    "```\n",
    "workflows/ragtime\n",
    "├── pyproject.toml\n",
    "└── src\n",
    "    └── ragtime\n",
    "        ├── __init__.py\n",
    "        ├── configs\n",
    "        │   └── config.yml\n",
    "        ├── ragtime_function.py\n",
    "        └── register.py\n",
    "```\n",
    "\n",
    "Let's create an additional directory to store our agent config (note that this does not need to be stored in the workflow directory, we're just doing it here for convenience):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p workflows/ragtime/configs\n",
    "!mkdir -p workflows/ragtime/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating the Package Configuration\n",
    "\n",
    "Now we can update the `pyproject.toml` file with specifics about our workflow. This file specifies:\n",
    "\n",
    "- Package metadata (name, version, description)\n",
    "- Dependencies (we'll use `agentiq[llama-index,langchain]` for RAG support and `colorama` for better output)\n",
    "- Entry point registration so AgentIQ can discover our components\n",
    "\n",
    "The entry point maps the 'agentiq_ragtime.register' module to the AgentIQ component system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/ragtime/pyproject.toml\n",
    "[build-system]\n",
    "build-backend = \"setuptools.build_meta\"\n",
    "requires = [\"setuptools >= 64\"]\n",
    "\n",
    "[project]\n",
    "name = \"ragtime\"\n",
    "version = \"0.1.0\"\n",
    "dependencies = [\n",
    "  \"agentiq[llama-index,langchain]\",\n",
    "  \"colorama\"\n",
    "]\n",
    "requires-python = \">=3.12\"\n",
    "description = \"RAG workflow for AgentIQ\"\n",
    "classifiers = [\"Programming Language :: Python\"]\n",
    "\n",
    "\n",
    "\n",
    "[project.entry-points.'aiq.components']\n",
    "ragtime = \"ragtime.register\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Basic Configuration Without RAG\n",
    "\n",
    "Before implementing RAG, let's create a basic configuration that uses a standard agent without retrieval capabilities. This will help us understand the baseline performance and see the impact of adding RAG later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating the Initial Configuration File\n",
    "\n",
    "Let's create a YAML configuration file that defines the core components of our workflow:\n",
    "\n",
    "- **general**: Basic settings like using uvloop for better performance\n",
    "- **llms**: Language model configuration (using NIM with Llama 3.1)\n",
    "- **workflow**: Agent type (ReAct) and connection to the LLM\n",
    "\n",
    "At this stage, we're not including any RAG-specific components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/ragtime/configs/basic_config.yml\n",
    "\n",
    "general:\n",
    "  use_uvloop: true\n",
    "  telemetry:\n",
    "    tracing:\n",
    "      phoenix:\n",
    "          _type: phoenix\n",
    "          endpoint: http://phoenix:6006/v1/traces\n",
    "          project: rag_example\n",
    "\n",
    "llms:\n",
    "  nim_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-70b-instruct\n",
    "    temperature: 0.0\n",
    "\n",
    "workflow:\n",
    "  _type: simple_llm_call\n",
    "  llm_name: nim_llm\n",
    "  verbose: true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Installing the Workflow\n",
    "\n",
    "Now let's install our workflow package using pip. This makes our custom components available to AgentIQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -e workflows/ragtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Testing the Basic Configuration\n",
    "\n",
    "Let's run our agent with a question about literature to see how it performs without RAG. We'll use a question about \"Anne of Green Gables\" since that's the document we'll be using for RAG later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aiq run --config_file workflows/ragtime/configs/basic_config.yml --input \"Who is Anne Shirley and what is her personality like?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model provides an answer based on its pre-trained knowledge. However, it might not have the most detailed or accurate information about specific aspects of \"Anne of Green Gables\". This is where RAG can help by providing the model with relevant passages from the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents for RAG\n",
    "\n",
    "For our RAG system to be effective, we need to prepare appropriate documents that will serve as our knowledge base. This course has a few books from Project Gutenberg which are in the public domain, downloaded in advance so we don't waste Project Gutenberg's bandwidth.\n",
    "\n",
    "We have the following titles:\n",
    "- Twenty Thousand Leagues under the Sea by Jules Verne\n",
    "- The War of the Worlds by H. G. Wells\n",
    "- Anne of Green Gables by Lucy Maud Montgomery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Listing Available Documents\n",
    "\n",
    "Run the following command to ensure the documents are ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the files in the rag_data directory\n",
    "!ls -al rag_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploring the Document\n",
    "\n",
    "Let's take a look at the beginning of one of the documents to understand its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first 1000 characters to see the structure\n",
    "with open(\"rag_data/anne_of_green_gables_project_gutenberg.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    preview = f.read(1000)\n",
    "    \n",
    "print(preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating Evaluation Questions\n",
    "\n",
    "Now, let's create a set of evaluation questions that we'll use to test our RAG system. These questions should be specific to the content of \"Anne of Green Gables\" so we can clearly see the difference between the model's pre-trained knowledge and information retrieved from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/ragtime/data/rag_questions.json\n",
    "[\n",
    "  {\n",
    "    \"id\": 1,\n",
    "    \"question\": \"What is the name of the book Anne is caught reading in class in Anne of Green Gables?\",\n",
    "    \"answer\": \"Ben Hur\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 2,\n",
    "    \"question\": \"What type of flowers does Anne place on Matthew's grave?\",\n",
    "    \"answer\": \"roses\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 3,\n",
    "    \"question\": \"What poem does Anne recite at the White Sands Hotel concert?\",\n",
    "    \"answer\": \"The Maiden's Vow\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 4,\n",
    "    \"question\": \"What was the color of the smoke that squirted from the joints of the martian machines?\",\n",
    "    \"answer\": \"green\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 5,\n",
    "    \"question\": \"What poisonous weapon did the aliens use to silence large amounts of artillery in The War of the Worlds?\",\n",
    "    \"answer\": \"black smoke\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 6,\n",
    "    \"question\": \"What is the name of the inventor of the projectile weapon used by Captain Nemo in Twenty Thousand Leagues Under the Sea?\",\n",
    "    \"answer\": \"Leniebroek\"\n",
    "  }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Creating an Evaluation Configuration\n",
    "\n",
    "Now, let's create an evaluation configuration file that we'll use to test our agent's performance with and without RAG.\n",
    "\n",
    "Note that we are using a larger LLM (Llama 3.1 405B) as the evaluation LLM. This is not always necessary, but this highlights that we can define multiple LLM components for different purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/ragtime/configs/eval_config.yml\n",
    "\n",
    "general:\n",
    "  use_uvloop: true\n",
    "  telemetry:\n",
    "    tracing:\n",
    "      phoenix:\n",
    "          _type: phoenix\n",
    "          endpoint: http://phoenix:6006/v1/traces\n",
    "          project: rag_example\n",
    "\n",
    "functions:\n",
    "  current_datetime:\n",
    "    _type: current_datetime\n",
    "\n",
    "llms:\n",
    "  nim_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-70b-instruct\n",
    "    temperature: 0.0\n",
    "  \n",
    "  nim_rag_eval_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-405b-instruct\n",
    "    temperature: 0.0\n",
    "\n",
    "workflow:\n",
    "  _type: simple_llm_call\n",
    "  llm_name: nim_llm\n",
    "  verbose: true\n",
    "\n",
    "eval:\n",
    "  general:\n",
    "    output_dir: ./ragtime_eval/\n",
    "    dataset:\n",
    "      _type: json\n",
    "      file_path: workflows/ragtime/data/rag_questions.json\n",
    "  evaluators:\n",
    "    rag_accuracy:\n",
    "      _type: ragas\n",
    "      metric: AnswerAccuracy\n",
    "      llm_name: nim_rag_eval_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Running the Baseline Evaluation\n",
    "\n",
    "Let's run an evaluation of our basic agent (without RAG) to establish a baseline performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aiq eval --config_file workflows/ragtime/configs/eval_config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Examining the Evaluation Results\n",
    "\n",
    "Let's look at the evaluation results to understand how well our agent performed without RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the workflow output\n",
    "with open(\"./ragtime_eval/workflow_output.json\", \"r\") as f:\n",
    "    workflow_output = json.load(f)\n",
    "\n",
    "# Load the evaluation results\n",
    "with open(\"./ragtime_eval/rag_accuracy_output.json\", \"r\") as f:\n",
    "    eval_results = json.load(f)\n",
    "\n",
    "print(f\"Average accuracy score: {eval_results['average_score']}\\n\")\n",
    "\n",
    "# Print questions, expected answers, and generated answers\n",
    "for i, item in enumerate(workflow_output):\n",
    "    print(f\"Question {i+1}: {item['question']}\")\n",
    "    print(f\"Expected: {item['answer']}\")\n",
    "    print(f\"Generated: {item['generated_answer']}\")\n",
    "    print(f\"Score: {eval_results['eval_output_items'][i]['score']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model's performance on these specific questions about the books is limited by its pre-trained knowledge. In fact, we have a very low score. Now, let's implement RAG to see if we can improve the accuracy by providing the model with relevant passages from the books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the RAG Workflow\n",
    "\n",
    "Now that we've established a baseline, let's implement a RAG workflow using AgentIQ and LlamaIndex. This will allow our agent to retrieve relevant information from \"Anne of Green Gables\" when answering questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating the LlamaIndex RAG Tool\n",
    "\n",
    "First, let's create the LlamaIndex RAG tool that will handle document ingestion, embedding, and retrieval. This tool will:\n",
    "1. Load the documents from our data directory\n",
    "2. Parse them into chunks\n",
    "3. Create embeddings for each chunk\n",
    "4. Build a vector index for efficient retrieval\n",
    "5. Provide a query interface for the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/ragtime/src/ragtime/ragtime_function.py\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from pydantic import ConfigDict\n",
    "\n",
    "from aiq.builder.builder import Builder\n",
    "from aiq.builder.framework_enum import LLMFrameworkEnum\n",
    "from aiq.builder.function_info import FunctionInfo\n",
    "from aiq.cli.register_workflow import register_function\n",
    "from aiq.data_models.component_ref import EmbedderRef\n",
    "from aiq.data_models.component_ref import LLMRef\n",
    "from aiq.data_models.function import FunctionBaseConfig\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class BookKnowledgeRAGConfig(FunctionBaseConfig, name=\"book_knowledge_rag\"):\n",
    "    data_dir: str\n",
    "    data_db_dir: str\n",
    "    chunk_size: int = 200\n",
    "    chunk_overlap: int = 50\n",
    "    top_k: int = 10\n",
    "    fetch_k: int = 20\n",
    "    rerank_model: str = \"nvidia/llama-3.2-nv-rerankqa-1b-v2\"\n",
    "    debug_mode: bool = False\n",
    "    model_config = ConfigDict(protected_namespaces=())\n",
    "    llm_name: LLMRef\n",
    "    embedding_name: EmbedderRef\n",
    "    query_response_mode: str = \"simple_summarize\"\n",
    "\n",
    "@register_function(config_type=BookKnowledgeRAGConfig, framework_wrappers=[LLMFrameworkEnum.LLAMA_INDEX])\n",
    "async def book_knowledge_rag_tool(tool_config: BookKnowledgeRAGConfig, builder: Builder):\n",
    "    \"\"\"\n",
    "    A RAG system for querying a collection of books.\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    from typing import List, Dict\n",
    "    from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, StorageContext, load_index_from_storage\n",
    "    from llama_index.core.node_parser import SentenceSplitter\n",
    "    from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "    import faiss\n",
    "    from llama_index.llms.nvidia import NVIDIA\n",
    "    from llama_index.embeddings.nvidia import NVIDIAEmbedding\n",
    "    from llama_index.postprocessor.nvidia_rerank import NVIDIARerank\n",
    "\n",
    "    # Get the LLM and embedder from the builder\n",
    "    llm = await builder.get_llm(tool_config.llm_name, wrapper_type=LLMFrameworkEnum.LLAMA_INDEX)\n",
    "    embedder = await builder.get_embedder(tool_config.embedding_name, wrapper_type=LLMFrameworkEnum.LLAMA_INDEX)\n",
    "\n",
    "    reranker = NVIDIARerank(model=tool_config.rerank_model, top_n=tool_config.top_k)\n",
    "    # NOTE: this workshop environment is sending API requests through a proxy and for LlamaIndex's reranker\n",
    "    # we need to set `base_url` explicity to use it. In your own environment, you would not need the following line.\n",
    "    reranker.base_url = \"http://proxy/v1/retrieval/nvidia/llama-3_2-nv-rerankqa-1b-v2/reranking\"\n",
    "    Settings.llm = llm\n",
    "    Settings.embed_model = embedder\n",
    "\n",
    "    # Validate data directory\n",
    "    data_dir_path = Path(tool_config.data_dir)\n",
    "    if not data_dir_path.exists() or not data_dir_path.is_dir():\n",
    "        raise ValueError(f\"Invalid or non-existent data directory: {tool_config.data_dir}\")\n",
    "    txt_files = list(data_dir_path.glob(\"*.txt\"))\n",
    "    if not txt_files:\n",
    "        raise ValueError(f\"No .txt files found in directory: {tool_config.data_dir}\")\n",
    "\n",
    "    # Prepare database directory\n",
    "    db_dir_path = Path(tool_config.data_db_dir)\n",
    "    if not db_dir_path.exists():\n",
    "        db_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Check if index exists\n",
    "    index_persisted = all(\n",
    "        db_dir_path.joinpath(f).exists() \n",
    "        for f in [\"default__vector_store.json\", \"docstore.json\", \"index_store.json\"]\n",
    "    )\n",
    "\n",
    "    if index_persisted:\n",
    "        vector_store = FaissVectorStore.from_persist_dir(persist_dir=str(db_dir_path))\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store, persist_dir=str(db_dir_path))\n",
    "        vector_index = load_index_from_storage(\n",
    "            storage_context=storage_context,\n",
    "            embed_model=embedder\n",
    "        )\n",
    "    else:\n",
    "        documents = SimpleDirectoryReader(\n",
    "            input_dir=str(data_dir_path),\n",
    "            required_exts=[\".txt\"],\n",
    "            recursive=True,\n",
    "            filename_as_id=True\n",
    "        ).load_data()\n",
    "\n",
    "        node_parser = SentenceSplitter(chunk_size=tool_config.chunk_size, chunk_overlap=tool_config.chunk_overlap)\n",
    "        nodes = node_parser.get_nodes_from_documents(documents)\n",
    "        for node in nodes:\n",
    "            node.metadata[\"book_title\"] = Path(node.metadata.get(\"file_name\", \"Unknown\")).name\n",
    "\n",
    "        # Create FAISS index\n",
    "        sample_embedding = embedder.get_text_embedding(\"test\")\n",
    "        embedding_dim = len(sample_embedding)\n",
    "        faiss_index = faiss.IndexFlatL2(embedding_dim)  # Flat L2 index\n",
    "        vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "        # Build and populate index\n",
    "        vector_index = VectorStoreIndex(\n",
    "            nodes=nodes,\n",
    "            storage_context=storage_context,\n",
    "            embed_model=embedder,\n",
    "            show_progress=True\n",
    "        )\n",
    "\n",
    "        # Persist the index (handled by llama_index)\n",
    "        vector_index.storage_context.persist(persist_dir=str(db_dir_path))\n",
    "\n",
    "    # Query engine setup\n",
    "    node_postprocessors = [reranker]\n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=tool_config.fetch_k,\n",
    "        response_mode=tool_config.query_response_mode,\n",
    "        verbose=tool_config.debug_mode,\n",
    "        node_postprocessors=node_postprocessors\n",
    "    )\n",
    "\n",
    "    async def _arun(question: str) -> str:\n",
    "        try:\n",
    "            # Query processing\n",
    "            response = await query_engine.aquery(question)\n",
    "            answer = str(response.response).strip()\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            return f\"Error: Failed to process query - {str(e)}\"\n",
    "\n",
    "    yield FunctionInfo.from_fn(_arun, description=\"Extract relevant information from books to answer questions efficiently\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we need to update the imports in `register.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/ragtime/src/ragtime/register.py\n",
    "# pylint: disable=unused-import\n",
    "# flake8: noqa\n",
    "\n",
    "# Import any tools which need to be automatically registered here\n",
    "from ragtime.ragtime_function import book_knowledge_rag_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reinstalling the Workflow\n",
    "\n",
    "Now that we've added our RAG components, let's reinstall the workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -e workflows/ragtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating the RAG Configuration\n",
    "\n",
    "Now, let's create a configuration file that includes our RAG components.\n",
    "\n",
    "Note that we are now using a `react_agent` that can call functions, namely our `book_knowledge_rag` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/ragtime/configs/rag_config.yml\n",
    "\n",
    "general:\n",
    "  use_uvloop: true\n",
    "  telemetry:\n",
    "    tracing:\n",
    "      phoenix:\n",
    "          _type: phoenix\n",
    "          endpoint: http://phoenix:6006/v1/traces\n",
    "          project: rag_example\n",
    "  \n",
    "llms:\n",
    "  nim_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-70b-instruct\n",
    "    temperature: 0.0\n",
    "    base_url: $NVIDIA_BASE_URL\n",
    "\n",
    "embedders:\n",
    "  nim_embedder:\n",
    "    _type: nim\n",
    "    model_name: nvidia/nv-embedqa-e5-v5\n",
    "    base_url: $NVIDIA_BASE_URL\n",
    "    truncate: END\n",
    "\n",
    "functions:\n",
    "  book_knowledge_rag:\n",
    "    _type: book_knowledge_rag\n",
    "    llm_name: nim_llm\n",
    "    embedding_name: nim_embedder\n",
    "    data_dir: rag_data\n",
    "    data_db_dir: rag_db\n",
    "\n",
    "workflow:\n",
    "  _type: react_agent\n",
    "  tool_names:\n",
    "    - book_knowledge_rag\n",
    "  llm_name: nim_llm\n",
    "  verbose: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Testing the RAG Workflow\n",
    "\n",
    "Let's test our RAG workflow with the same question we used earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aiq run --config_file workflows/ragtime/configs/rag_config.yml --input \"Tell me a little about the personality of Anne Shirley.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the response now includes specific details from the book that weren't in the model's pre-trained knowledge. The RAG system has successfully retrieved relevant passages and used them to enhance the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Creating a RAG Evaluation Configuration\n",
    "\n",
    "Now, let's create an evaluation configuration for our RAG workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/ragtime/configs/rag_eval_config.yml\n",
    "\n",
    "general:\n",
    "  use_uvloop: true\n",
    "  telemetry:\n",
    "    tracing:\n",
    "      phoenix:\n",
    "          _type: phoenix\n",
    "          endpoint: http://phoenix:6006/v1/traces\n",
    "          project: rag_example\n",
    "\n",
    "llms:\n",
    "  nim_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-70b-instruct\n",
    "    temperature: 0.0\n",
    "    base_url: $NVIDIA_BASE_URL\n",
    "  \n",
    "  nim_rag_eval_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-405b-instruct\n",
    "    temperature: 0.0\n",
    "    base_url: $NVIDIA_BASE_URL\n",
    "\n",
    "embedders:\n",
    "  nim_embedder:\n",
    "    _type: nim\n",
    "    model_name: nvidia/nv-embedqa-e5-v5\n",
    "    base_url: $NVIDIA_BASE_URL\n",
    "    truncate: END\n",
    "\n",
    "functions:\n",
    "  book_knowledge_rag:\n",
    "    _type: book_knowledge_rag\n",
    "    llm_name: nim_llm\n",
    "    embedding_name: nim_embedder\n",
    "    data_dir: rag_data\n",
    "    data_db_dir: rag_db\n",
    "\n",
    "workflow:\n",
    "  _type: react_agent\n",
    "  tool_names:\n",
    "    - book_knowledge_rag\n",
    "  llm_name: nim_llm\n",
    "  verbose: true\n",
    "\n",
    "eval:\n",
    "  general:\n",
    "    output_dir: ./ragtime_eval/\n",
    "    dataset:\n",
    "      _type: json\n",
    "      file_path: workflows/ragtime/data/rag_questions.json\n",
    "  evaluators:\n",
    "    rag_accuracy:\n",
    "      _type: ragas\n",
    "      metric: AnswerAccuracy\n",
    "      llm_name: nim_rag_eval_llm\n",
    "    rag_groundedness:\n",
    "      _type: ragas\n",
    "      metric: ResponseGroundedness\n",
    "      llm_name: nim_rag_eval_llm\n",
    "    rag_relevance:\n",
    "      _type: ragas\n",
    "      metric: ContextRelevance\n",
    "      llm_name: nim_rag_eval_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Running the RAG Evaluation\n",
    "\n",
    "Let's run the evaluation on our RAG workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aiq eval --config_file workflows/ragtime/configs/rag_eval_config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Comparing Evaluation Results\n",
    "\n",
    "Now, let's compare the evaluation results between the non-RAG and RAG workflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the RAG evaluation results\n",
    "with open(\"./ragtime_eval/rag_accuracy_output.json\", \"r\") as f:\n",
    "    rag_results = json.load(f)\n",
    "\n",
    "print(f\"RAG average accuracy: {rag_results['average_score']}\\n\")\n",
    "\n",
    "# Load the groundedness and relevance results for RAG\n",
    "with open(\"./ragtime_eval/rag_groundedness_output.json\", \"r\") as f:\n",
    "    groundedness_results = json.load(f)\n",
    "\n",
    "with open(\"./ragtime_eval/rag_relevance_output.json\", \"r\") as f:\n",
    "    relevance_results = json.load(f)\n",
    "\n",
    "print(f\"RAG groundedness: {groundedness_results['average_score']}\")\n",
    "print(f\"RAG relevance: {relevance_results['average_score']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the RAG workflow outperforms the non-RAG workflow (which had an average accuracy of 0.20) on our evaluation questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've built and enhanced a RAG workflow using AgentIQ that effectively retrieves and processes information from a library of books. We've covered:\n",
    "\n",
    "1. Setting up the basic RAG infrastructure\n",
    "2. Implementing document processing and retrieval\n",
    "3. Creating an evaluation framework\n",
    "\n",
    "This implementation serves as a foundation for building more sophisticated RAG systems with AgentIQ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
