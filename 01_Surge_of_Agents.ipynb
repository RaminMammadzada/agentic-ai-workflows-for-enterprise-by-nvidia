{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Surge of Agents: A Comprehensive Tour of Modern Agentic Frameworks\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to this hands-on exploration of cutting-edge agentic frameworks! In this notebook, we'll dissect the architecture and capabilities of four powerful frameworks that are revolutionizing how we build intelligent systems. Agentic systems—computational entities that can perceive, decide, and act autonomously—represent a paradigm shift in AI development, enabling more sophisticated reasoning, planning, and problem-solving capabilities.\n",
    "\n",
    "We'll use a common task—generating and solving mathematical word problems—to showcase each framework's unique approach and strengths:\n",
    "\n",
    "- **OpenAI Python Library**: The foundation of many agentic systems, providing direct access to state-of-the-art language models with a clean, intuitive API. We'll see how even with minimal scaffolding, powerful agents can be constructed.\n",
    "\n",
    "- **LangChain**: A comprehensive toolkit that abstracts away complexity in prompt engineering, output parsing, and workflow composition. We'll explore how it enables structured data handling through Pydantic integration and composable processing chains.\n",
    "\n",
    "- **CrewAI**: A specialized framework for multi-agent orchestration that models complex systems as collaborative teams with distinct roles, goals, and capabilities. We'll examine how it facilitates agent specialization and structured task delegation.\n",
    "\n",
    "- **LangGraph**: An emerging framework focused on graph-based workflow management that excels at modeling complex, non-linear interaction patterns. We'll demonstrate its power in creating modular, flexible agent architectures.\n",
    "\n",
    "By the end of this notebook, you'll understand the distinctive design philosophies of these frameworks and be equipped to select the right tool for your specific agentic system needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "Before diving into the frameworks, we need to establish our development environment. We'll configure access to the NVIDIA AI Foundation Models platform, which provides access to powerful open-source models like Llama 3.1.\n",
    "\n",
    "### Key Configuration Elements:\n",
    "\n",
    "- **API Key**: The authentication token required to access NVIDIA's API services. In production environments, this should be stored securely as an environment variable rather than hardcoded in your notebooks. In this workshop environment we are providing an API key for your use.\n",
    "\n",
    "- **Endpoint URL**: The base URL that directs our requests to NVIDIA's AI model serving infrastructure. This endpoint handles all communication between our code and the foundation models. Typically this endpoint URL is `\"https://integrate.api.nvidia.com/v1\"`, however, in this workshop environment we are sending all calls to NVIDIA's API service through a proxy that we are managing.\n",
    "\n",
    "- **Model Selection**: We're using `meta/llama-3.1-70b-instruct`, a powerful open-source LLM that balances performance and efficiency contained in an NVIDIA NIM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by setting up these configuration parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# In your own environemnt the `endpoint_url` should be https://integrate.api.nvidia.com/v1\". Here we set it to a proxy\n",
    "# service we use in this workshop environment.\n",
    "endpoint_url = os.getenv(\"NVIDIA_BASE_URL\")\n",
    "api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
    "model_name = \"meta/llama-3.1-70b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining Your Own NVIDIA API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have an NVIDIA API key and need one for work in your own environment, you can generate one for free using the following steps:\n",
    "\n",
    "1. Login (or sign up) through [build.nvidia.com](https://build.nvidia.com/explore/discover).\n",
    "2. Click the `Get API Key` button available on the the `meta/llama-3_1-70b-instruct` page, found [here](https://build.nvidia.com/meta/llama-3_1-70b-instruct)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework 1: The OpenAI Python Library\n",
    "\n",
    "The OpenAI Python library provides a clean, straightforward interface to language models. While its name suggests exclusivity to OpenAI's models, this library can be configured to work with alternative endpoints, as we're doing with NVIDIA's API.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Client Initialization**: We configure the client with our API key, organization, and custom endpoint URL\n",
    "2. **Completion Creation**: The primary method for generating text is through the `chat.completions.create()` method\n",
    "3. **Message Structure**: Inputs are formatted as a list of message objects with roles (system, user, assistant) and content\n",
    "4. **Response Handling**: Outputs are structured objects containing generated text and metadata\n",
    "\n",
    "### Talking to an LLM:\n",
    "\n",
    "Here, we will generates a math word problem about pre-algebra.\n",
    "\n",
    "Depending on your definition of agent, the single call to the LLM in the next cell is not agentic. However, the OpenAI library is powerful, and calls could be chained together to create an agentic framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Equation:\n",
      "--------------------------------------------------\n",
      "2x + 7 = 19\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the client with our configuration\n",
    "openai = OpenAI(\n",
    "    organization=\"nvidia\",\n",
    "    api_key=api_key,\n",
    "    base_url=endpoint_url,\n",
    ")\n",
    "\n",
    "        \n",
    "difficulty = \"pre-algebra\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Create a math equation suitable for a {difficulty} student that involves solving for a single variable, x.\n",
    "Use integers and basic operations (e.g., addition, subtraction, multiplication).\n",
    "Provide only the equation, like \"3x - 5 = 10\".\n",
    "\"\"\"\n",
    "\n",
    "# Generate the equation\n",
    "response = openai.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "# Extract the generated problem from the response\n",
    "equation_text = response.choices[0].message.content\n",
    "\n",
    "print(\"Generated Equation:\")\n",
    "print(\"-\" * 50)\n",
    "print(equation_text)\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis - OpenAI Python Library\n",
    "\n",
    "- **Minimal Setup**: Just a few lines of code to get started\n",
    "- **Direct Control**: Low-level access to the model's capabilities\n",
    "- **Flexibility**: Can be used with any compatible API endpoint\n",
    "\n",
    "The OpenAI library provides a solid foundation for sending and receiving data from LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework 2: LangChain - Composition and Structure\n",
    "\n",
    "LangChain provides abstractions for composing multi-step workflows and handling structured outputs. It's designed to make complex agent patterns more manageable through reusable components.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Prompt Templates**: Parameterized text templates that can be reused across different contexts\n",
    "2. **Output Parsers**: Specialized components that transform unstructured LLM outputs into structured data objects\n",
    "3. **Chains**: Composable sequences of operations that can be executed as a single unit\n",
    "4. **Pydantic Integration**: Using Python's type system to validate and structure data\n",
    "\n",
    "### Building a Structured Agent:\n",
    "\n",
    "We'll take the math problem generated earlier and build a word problem for it\n",
    "1. Uses typed templates to generate the problem\n",
    "2. Parses the output into a structured format using Pydantic models\n",
    "3. Chains operations together using LangChain's pipeline operator (`|`)\n",
    "\n",
    "This demonstrates how LangChain promotes robust and maintainable agent architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"equation\": \"2x + 7 = 19\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"equation\": \"2x + 7 = 19\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatNVIDIA] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Given the equation 2x + 7 = 19, create a realistic pre-algebra word problem that matches it.\\n    The problem should involve a real-world scenario (e.g., shopping, travel) and require solving for x.\\n    Provide only the word problem.\\n    Format your response as JSON: The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\\\"properties\\\": {\\\"foo\\\": {\\\"title\\\": \\\"Foo\\\", \\\"description\\\": \\\"a list of strings\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}, \\\"required\\\": [\\\"foo\\\"]}\\nthe object {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]} is a well-formatted instance of the schema. The object {\\\"properties\\\": {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\\\"properties\\\": {\\\"word_problem\\\": {\\\"description\\\": \\\"The text of the math word problem\\\", \\\"title\\\": \\\"Word Problem\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"word_problem\\\"]}\\n```. Do not include any other text but the JSON.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatNVIDIA] [1.50s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"word_problem\\\": \\\"Tom has been saving money for a new bike and has $7 in his piggy bank. He plans to mow lawns to earn more money and expects to earn $2 for each lawn he mows. If he wants to buy a bike that costs $19, how many lawns (x) does Tom need to mow to have enough money to buy the bike?\\\"}\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"word_problem\\\": \\\"Tom has been saving money for a new bike and has $7 in his piggy bank. He plans to mow lawns to earn more money and expects to earn $2 for each lawn he mows. If he wants to buy a bike that costs $19, how many lawns (x) does Tom need to mow to have enough money to buy the bike?\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"role\": \"assistant\",\n",
      "              \"content\": \"{\\\"word_problem\\\": \\\"Tom has been saving money for a new bike and has $7 in his piggy bank. He plans to mow lawns to earn more money and expects to earn $2 for each lawn he mows. If he wants to buy a bike that costs $19, how many lawns (x) does Tom need to mow to have enough money to buy the bike?\\\"}\",\n",
      "              \"token_usage\": {\n",
      "                \"prompt_tokens\": 241,\n",
      "                \"total_tokens\": 324,\n",
      "                \"completion_tokens\": 83\n",
      "              },\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"model_name\": \"meta/llama-3.1-70b-instruct\"\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--c190a7ab-0f75-4001-a314-5a24ba72de5d-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 241,\n",
      "              \"output_tokens\": 83,\n",
      "              \"total_tokens\": 324\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"{\\\"word_problem\\\": \\\"Tom has been saving money for a new bike and has $7 in his piggy bank. He plans to mow lawns to earn more money and expects to earn $2 for each lawn he mows. If he wants to buy a bike that costs $19, how many lawns (x) does Tom need to mow to have enough money to buy the bike?\\\"}\",\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 241,\n",
      "      \"total_tokens\": 324,\n",
      "      \"completion_tokens\": 83\n",
      "    },\n",
      "    \"finish_reason\": \"stop\",\n",
      "    \"model_name\": \"meta/llama-3.1-70b-instruct\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:PydanticOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:PydanticOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [1.50s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "Generated Equation:\n",
      "--------------------------------------------------\n",
      "2x + 7 = 19\n",
      "--------------------------------------------------\n",
      "\n",
      "Word Problem:\n",
      "--------------------------------------------------\n",
      "Tom has been saving money for a new bike and has $7 in his piggy bank. He plans to mow lawns to earn more money and expects to earn $2 for each lawn he mows. If he wants to buy a bike that costs $19, how many lawns (x) does Tom need to mow to have enough money to buy the bike?\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#  LangChain Implementation\n",
    "from langchain import PromptTemplate\n",
    "from langchain.globals import set_debug  # Enables detailed logging of chain execution\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field  # For structured data validation\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "# Enable debug mode to see the full chain execution details\n",
    "set_debug(True)\n",
    "\n",
    "# Initialize our LLM with the NVIDIA endpoint\n",
    "llm = ChatNVIDIA(model=model_name, base_url=endpoint_url)\n",
    "\n",
    "# Define a structured data model for word problems\n",
    "class WordProblem(BaseModel):\n",
    "    word_problem: str = Field(description=\"The text of the math word problem\")\n",
    "\n",
    "# Create a parser that will extract structured data from LLM responses\n",
    "word_problem_parser = PydanticOutputParser(pydantic_object=WordProblem)\n",
    "\n",
    "# Define a template for generating word problems with instructions for proper formatting\n",
    "word_problem_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given the equation {equation}, create a realistic pre-algebra word problem that matches it.\n",
    "    The problem should involve a real-world scenario (e.g., shopping, travel) and require solving for x.\n",
    "    Provide only the word problem.\n",
    "    Format your response as JSON: {format_instructions}. Do not include any other text but the JSON.\"\"\",\n",
    "    partial_variables={\"format_instructions\": word_problem_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# Compose the entire workflow as a chain using the pipeline operator\n",
    "chain = word_problem_prompt | llm | word_problem_parser\n",
    "\n",
    "# Execute the full chain with a single call\n",
    "result = chain.invoke({\"equation\": equation_text})\n",
    "\n",
    "print(\"Generated Equation:\") # from the first agent\n",
    "print(\"-\" * 50)\n",
    "print(equation_text)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "word_problem = result.word_problem\n",
    "print(\"\\nWord Problem:\")\n",
    "print(\"-\" * 50)\n",
    "print(word_problem)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis - LangChain\n",
    "\n",
    "#### Strengths:\n",
    "- **Structured Data Handling**: Pydantic integration ensures typed, validated outputs\n",
    "- **Composable Patterns**: The pipeline operator (`|`) enables clean workflow composition\n",
    "- **Reusable Components**: Templates and parsers can be shared across multiple agents\n",
    "- **Simplified Chaining**: Automatic passing of outputs between steps\n",
    "\n",
    "LangChain excels at creating structured, maintainable workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework 3: LangGraph - Graph-Based Workflow Management\n",
    "\n",
    "LangGraph represents the cutting edge of agentic workflow management, using directed graphs to model complex interactions between components. This approach offers maximum flexibility for creating sophisticated, non-linear agent architectures.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Nodes**: Discrete processing units that perform specific functions\n",
    "2. **Edges**: Connections between nodes that define data flow and execution order\n",
    "3. **Graphs**: Complete workflow definitions with nodes and edges\n",
    "4. **State Management**: Tracking and updating context throughout execution\n",
    "\n",
    "### Building a Graph-Based Agent:\n",
    "\n",
    "We'll create a modular workflow that:\n",
    "1. Defines distinct nodes for problem generation and solving\n",
    "2. Establishes connections between nodes to control data flow\n",
    "3. Executes the graph to process our mathematical tasks\n",
    "\n",
    "This demonstrates LangGraph's power for creating flexible, maintainable agent architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langgraph.graph import Graph, START\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "# Enable detailed logging\n",
    "set_debug(True)\n",
    "\n",
    "# Initialize our LLM\n",
    "llm = ChatNVIDIA(model=model_name, base_url=endpoint_url)\n",
    "\n",
    "# Create prompts\n",
    "equation_solver_prompt = PromptTemplate(\n",
    "        input_variables=[\"equation\", \"word_problem\"],\n",
    "        template=\"\"\"Given the equation {equation} and matching word problem {word_problem}, solve it by providing only the mathematical steps as a list.\n",
    "        Each part should be a single equation or expression, showing the progression to the final solution, without any explanatory text. For example, for \"5 + x = 13\", output:\n",
    "        5 + x = 13 -> x = 13 - 5 -> x = 8\"\"\"\n",
    "    )\n",
    "    \n",
    "equation_solution_explainer_prompt = PromptTemplate(\n",
    "    input_variables=[\"equation\", \"word_problem\", \"solution\"],\n",
    "    template=\"\"\"Given the equation {equation}, the matching word problem {word_problem}, and the solution {solution}, explain the solution in plain English using the fewest words possible.\"\"\"\n",
    ")\n",
    "\n",
    "# Create chains\n",
    "equation_solver_chain = equation_solver_prompt | llm\n",
    "equation_solution_explainer_chain = equation_solution_explainer_prompt | llm\n",
    "\n",
    "# Define node functions\n",
    "def equation_solver_node(input_dict):\n",
    "    equation = input_dict[\"equation\"]\n",
    "    word_problem = input_dict[\"word_problem\"]\n",
    "    solution = equation_solver_chain.invoke({\"equation\": equation, \"word_problem\": word_problem})\n",
    "    # Ensure solution is a string (extract content if it's an AIMessage)\n",
    "    if hasattr(solution, 'content'):\n",
    "        solution = solution.content\n",
    "    return {\"solution\": solution, \"equation\": equation, \"word_problem\": word_problem}\n",
    "\n",
    "def equation_solution_explainer_node(input_dict):\n",
    "    equation = input_dict[\"equation\"]\n",
    "    word_problem = input_dict[\"word_problem\"]\n",
    "    solution = input_dict[\"solution\"]\n",
    "    explanation = equation_solution_explainer_chain.invoke({\"equation\": equation, \"word_problem\": word_problem, \"solution\": solution})\n",
    "    # Ensure explanation is a string (extract content if it's an AIMessage)\n",
    "    if hasattr(explanation, 'content'):\n",
    "        explanation = explanation.content\n",
    "    return {\"explanation\": explanation, \"equation\": equation, \"word_problem\": word_problem, \"solution\": solution}\n",
    "\n",
    "# Create our workflow graph\n",
    "graph = Graph()\n",
    "\n",
    "# Add our processing nodes\n",
    "graph.add_node(\"Solve Equation\", equation_solver_node)\n",
    "graph.add_node(\"Explain Solution\", equation_solution_explainer_node)\n",
    "\n",
    "# Define the flow between nodes\n",
    "graph.add_edge(START, \"Solve Equation\")\n",
    "graph.add_edge(\"Solve Equation\", \"Explain Solution\")\n",
    "\n",
    "# Set the finish point to the last node so its output is returned\n",
    "graph.set_finish_point(\"Explain Solution\")\n",
    "\n",
    "# Compile the graph into a runnable workflow\n",
    "workflow = graph.compile()\n",
    "\n",
    "# Run the workflow with our input data\n",
    "workflow_result = workflow.invoke({\n",
    "    \"equation\": equation_text,\n",
    "    \"word_problem\": word_problem\n",
    "})\n",
    "\n",
    "# Print the workflow result to debug\n",
    "print(\"Workflow Result:\")\n",
    "print(workflow_result)\n",
    "\n",
    "# Check if workflow_result is valid before proceeding\n",
    "if workflow_result is None:\n",
    "    print(\"Error: Workflow returned None. Check node execution or LLM invocation.\")\n",
    "else:\n",
    "    explanation = workflow_result.get('explanation', \"No explanation available\")\n",
    "    solution = workflow_result.get('solution', \"No solution available\")\n",
    "    # Display the results in a formatted way\n",
    "    display(Markdown(f\"\"\"\n",
    "    ### Equation\n",
    "    {equation_text}\n",
    "\n",
    "    ### Word Problem\n",
    "    {word_problem}\n",
    "\n",
    "    ### Step-by-Step Solution\n",
    "    {solution}\n",
    "\n",
    "    ### Explanation\n",
    "    {explanation}\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis - LangGraph\n",
    "\n",
    "#### Strengths:\n",
    "- **Maximum Flexibility**: Graph structures can represent arbitrary workflow patterns\n",
    "- **Modular Design**: Clear separation of concerns with independent nodes\n",
    "- **Transparent Data Flow**: Explicit edges show exactly how information passes between components\n",
    "- **Scalability**: Complex architectures remain manageable through graph visualization\n",
    "\n",
    "LangGraph provides the most power and flexibility among the frameworks we've explored, making it ideal for complex agent architectures with sophisticated reasoning patterns and state management needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework 4: CrewAI - Collaborative Multi-Agent Orchestration\n",
    "\n",
    "CrewAI takes a different approach by modeling agentic systems as teams (\"crews\") of specialized agents with distinct roles, goals, and capabilities. This framework is particularly well-suited for complex tasks that benefit from division of labor and specialized expertise.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Agents**: Entities with defined roles, goals, and backstories that shape their behavior\n",
    "2. **Tasks**: Units of work with descriptions and expected outputs\n",
    "3. **Crews**: Collections of agents working together in a coordinated process\n",
    "4. **Process Models**: Different approaches to task sequencing (sequential, hierarchical, etc.)\n",
    "\n",
    "### Building a Collaborative Agent Team:\n",
    "\n",
    "We'll create a two-agent system that:\n",
    "1. Uses a specialized \"Word Problem Generator\" agent to create challenging problems\n",
    "2. Delegates problem-solving to a \"Math Solver\" agent with mathematical expertise\n",
    "3. Coordinates their collaboration through a sequential workflow\n",
    "\n",
    "This demonstrates how CrewAI enables specialization through role definitions and coordinated execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, LLM\n",
    "\n",
    "# Initialize the LLM with the correct format for CrewAI\n",
    "llm = LLM(\n",
    "    model=f\"nvidia_nim/{model_name}\", base_url=endpoint_url, api_key=api_key\n",
    ")\n",
    "\n",
    "# Agent 1: Accuracy Checker\n",
    "accuracy_checker_agent = Agent(\n",
    "    role=\"Accuracy Checker\",\n",
    "    goal=\"Verify the mathematical correctness of the word problem, equation, and solution steps\",\n",
    "    backstory=\"You are a meticulous mathematician with a keen eye for detail. Your expertise lies in ensuring that every calculation and logical step in a math problem is correct, leaving no room for errors. You double-check solutions against the original problem to confirm accuracy.\",\n",
    "    llm=llm,\n",
    "    verbose=True  # Enable detailed logging of agent actions\n",
    ")\n",
    "\n",
    "# Define the accuracy checking task\n",
    "accuracy_task = Task(\n",
    "    description=\"Review the following: word problem '{word_problem}', equation '{equation}', and solution '{solution}'. Verify that the solution steps correctly solve the equation and match the word problem. Output 'Correct' if accurate, or identify any errors if incorrect.\",\n",
    "    expected_output=\"A concise statement confirming accuracy ('Correct') or detailing any errors found.\",\n",
    "    agent=accuracy_checker_agent,\n",
    ")\n",
    "\n",
    "# Agent 2: Clarity Reviewer\n",
    "clarity_reviewer_agent = Agent(\n",
    "    role=\"Clarity Reviewer\",\n",
    "    goal=\"Ensure the word problem and solution explanation are clear, engaging, and educationally valuable for students\",\n",
    "    backstory=\"You are an experienced educator with a passion for making math accessible and engaging. You excel at evaluating whether problems and explanations are easy to understand, appropriately challenging, and relevant to students’ learning needs.\",\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Define the clarity review task\n",
    "clarity_task = Task(\n",
    "    description=\"Review the following: word problem '{word_problem}' and solution explanation '{explanation}'. Assess if they are clear, engaging, and suitable for middle school students. Provide feedback, including at least one suggestion for improvement if applicable.\",\n",
    "    expected_output=\"A brief assessment of clarity and educational value, plus one suggestion for enhancement.\",\n",
    "    agent=clarity_reviewer_agent,\n",
    ")\n",
    "\n",
    "# Create a crew with both agents and their tasks\n",
    "crew = Crew(\n",
    "    agents=[accuracy_checker_agent, clarity_reviewer_agent], \n",
    "    tasks=[accuracy_task, clarity_task], \n",
    "    process=\"sequential\",  # Tasks will be executed in order \n",
    "    verbose=True  # Enable detailed logging of crew coordination\n",
    ")\n",
    "\n",
    "# Example inputs from previous pipeline stages\n",
    "inputs = {\n",
    "    \"word_problem\": word_problem,\n",
    "    \"equation\": equation_text,\n",
    "    \"solution\": solution,\n",
    "    \"explanation\": explanation\n",
    "}\n",
    "\n",
    "# Execute the full workflow\n",
    "result = crew.kickoff(inputs=inputs)\n",
    "\n",
    "# Display the result\n",
    "print(\"\\nCrewAI Result:\")\n",
    "print(\"-\" * 50)\n",
    "print(result)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis - CrewAI\n",
    "\n",
    "#### Strengths:\n",
    "- **Role-Based Design**: Agents can be specialized with distinct capabilities and knowledge\n",
    "- **Explicit Goals**: Each agent has clear objectives that guide its behavior\n",
    "- **Narrative Elements**: Backstories help shape agent personalities and approaches\n",
    "- **Flexible Coordination**: Multiple process models for different collaboration patterns\n",
    "\n",
    "CrewAI excels at modeling collaborative agent teams.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
