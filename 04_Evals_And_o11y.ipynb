{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating and Profiling AgentIQ Workflows\n",
    "\n",
    "You will walk through how to setup agent evaluation and observability in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Our Agent\n",
    "\n",
    "A key component of AgentIQ is that it can run several well-known evaluations against agentic workflows. Proper evaluation helps us:\n",
    "\n",
    "### 1. Why Evaluate?\n",
    "- **Measure Performance**: Quantify how well our agent performs on specific tasks\n",
    "- **Identify Weaknesses**: Find edge cases or failure modes\n",
    "- **Compare Versions**: Track improvements across different iterations\n",
    "- **Ensure Reliability**: Verify the agent works consistently\n",
    "\n",
    "### 2. Evaluation Process\n",
    "1. **Create Test Data**: Define questions with known answers\n",
    "2. **Configure Evaluators**: Set up metrics to measure performance\n",
    "3. **Run Evaluation**: Process all test cases\n",
    "4. **Analyze Results**: Review metrics and identify areas for improvement\n",
    "\n",
    "### 3. Available Metrics\n",
    "- **Answer Accuracy**: How correct are the agent's responses?\n",
    "- **Context Relevance**: Is the agent using appropriate context?\n",
    "- **Response Groundedness**: Are responses based on retrieved information?\n",
    "- **Trajectory Analysis**: Is the agent's reasoning process sound?\n",
    "\n",
    "This notebook will show how to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Methods in AgentIQ\n",
    "\n",
    "AgentIQ provides several built-in evaluators to assess the performance of your workflows:\n",
    "\n",
    "1. **RAGAS Evaluator**: An open-source evaluation framework for RAG (Retrieval-Augmented Generation) workflows. RAGAS provides metrics like Answer Accuracy, Context Relevance, and Response Groundedness.\n",
    "\n",
    "2. **Trajectory Evaluator**: Uses the intermediate steps generated by the workflow to evaluate the agent's reasoning process and decision-making path.\n",
    "\n",
    "3. **SWE-Bench Evaluator**: Specifically designed for software engineering tasks, this evaluator tests if the agent can solve programming problems by running tests on the generated code.\n",
    "\n",
    "In this notebook, we'll primarily use the **RAGAS Evaluator** and **Trajectory Evaluator** to assess our math tools agent.\n",
    "\n",
    "Let's create a directory to store evaluation data. This will contain test cases with questions and expected answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p workflows/math_tools/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we will create an evaluation JSON file.\n",
    "\n",
    "It will include both standard and time-aware test cases.\n",
    "\n",
    "Note that each test case includes the following:\n",
    "- id: A unique identifier\n",
    "- question: The input to send to our agent\n",
    "- answer: The expected correct response (or \"dynamic\" for time-based answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/math_tools/data/comprehensive_eval.json\n",
    "\n",
    "[\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"question\": \"What is the square root of 49?\",\n",
    "        \"answer\": \"7\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"question\": \"Add 10 to 25\",\n",
    "        \"answer\": \"35\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"question\": \"What is the modulus of 100 divided by 3?\",\n",
    "        \"answer\": \"1\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"question\": \"What is five to the power of three?\",\n",
    "        \"answer\": \"125\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"question\": \"Is the current hour even?\",\n",
    "        \"answer\": \"dynamic\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Comprehensive Evaluation Configuration\n",
    "\n",
    "We will write a single evaluation configuration file that includes all the tools our agent needs to handle both mathematical operations and time-based queries. By now you have seen most this configuration format. This configuration includes:\n",
    "\n",
    "1. **General settings**: Where to store results and which dataset to use\n",
    "2. **Functions**: All the tools our agent will use (math operations and time functions)\n",
    "3. **LLMs**: Both the agent LLM and a separate evaluation LLM\n",
    "4. **Evaluators**: The specific metrics we want to measure\n",
    "\n",
    "The `eval` section is new. In the `eval` section, you can specify however many evaluators you want to run, calling either built-in evaluators or your own custom evaluation components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/math_tools/configs/comprehensive_eval_config.yml\n",
    "\n",
    "general:\n",
    "  use_uvloop: true\n",
    "\n",
    "functions:\n",
    "  calculator_exponent:\n",
    "    _type: calculator_exponent\n",
    "  calculator_modulus:\n",
    "    _type: calculator_modulus\n",
    "  calculator_square_root:\n",
    "    _type: calculator_square_root\n",
    "  calculator_add:\n",
    "    _type: calculator_add\n",
    "  current_datetime:\n",
    "    _type: current_datetime\n",
    "\n",
    "llms:\n",
    "  nim_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-70b-instruct\n",
    "    temperature: 0.0\n",
    "  eval_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-405b-instruct\n",
    "    temperature: 0.0\n",
    "    max_tokens: 1024\n",
    "\n",
    "workflow:\n",
    "  _type: react_agent\n",
    "  tool_names:\n",
    "    - calculator_exponent\n",
    "    - calculator_modulus\n",
    "    - calculator_square_root\n",
    "    - calculator_add\n",
    "    - current_datetime\n",
    "  llm_name: nim_llm\n",
    "  verbose: true\n",
    "\n",
    "eval:\n",
    "  general:\n",
    "    output_dir: ./math_tools_eval/\n",
    "    dataset:\n",
    "      _type: json\n",
    "      file_path: workflows/math_tools/data/comprehensive_eval.json\n",
    "  evaluators:\n",
    "    math_accuracy:\n",
    "      _type: ragas\n",
    "      metric: AnswerAccuracy\n",
    "      llm_name: eval_llm\n",
    "    math_trajectory_accuracy:\n",
    "      _type: trajectory\n",
    "      llm_name: eval_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Comprehensive Evaluation\n",
    "\n",
    "Now let's run the evaluation using our consolidated configuration. This will:\n",
    "1. Load our test cases from the JSON file\n",
    "2. Run each test case through our agent\n",
    "3. Evaluate the responses using the specified metrics\n",
    "4. Store the results in the output directory\n",
    "\n",
    "This single evaluation run will test both standard mathematical operations and time-based queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aiq eval --config_file=workflows/math_tools/configs/comprehensive_eval_config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Evaluation Results\n",
    "\n",
    "After running the evaluation, AgentIQ stores the results in JSON files in our specified `output_dir`. Let's analyze these JSON results with some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Simple function to load JSON files\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Simple summary for accuracy evaluations\n",
    "def get_accuracy_summary(data):\n",
    "    items = data['eval_output_items']\n",
    "    summary = []\n",
    "    for item in items:\n",
    "        summary.append({\n",
    "            'Question': item['reasoning']['user_input'],\n",
    "            'Score': item['score']\n",
    "        })\n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "# Simple summary for workflow\n",
    "def get_workflow_summary(data):\n",
    "    summary = []\n",
    "    for item in data:\n",
    "        tools = []\n",
    "        total_tokens = 0\n",
    "        steps = len(item['intermediate_steps'])\n",
    "        \n",
    "        # Count tools and tokens\n",
    "        for step in item['intermediate_steps']:\n",
    "            if 'payload' in step and 'name' in step['payload']:\n",
    "                tools.append(step['payload']['name'])\n",
    "            if 'payload' in step and 'usage_info' in step['payload']:\n",
    "                total_tokens += step['payload']['usage_info']['token_usage']['total_tokens']\n",
    "        \n",
    "        summary.append({\n",
    "            'Question': item['question'],\n",
    "            'Steps': steps,\n",
    "            'Tokens': total_tokens,\n",
    "            'Tools': ', '.join(set(tools))  # Unique tools only\n",
    "        })\n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "# Load the three files\n",
    "accuracy = load_json('./math_tools_eval/math_accuracy_output.json')\n",
    "trajectory = load_json('./math_tools_eval/math_trajectory_accuracy_output.json')\n",
    "workflow = load_json('./math_tools_eval/workflow_output.json')\n",
    "\n",
    "# Create simple DataFrames\n",
    "accuracy_df = get_accuracy_summary(accuracy)\n",
    "workflow_df = get_workflow_summary(workflow)\n",
    "\n",
    "# Show basic metrics\n",
    "print(\"Overall Scores:\")\n",
    "print(f\"Math Accuracy Score: {accuracy['average_score']}\")\n",
    "print(f\"Trajectory Accuracy Score: {trajectory['average_score']}\")\n",
    "print(f\"Average Steps: {workflow_df['Steps'].mean()}\")\n",
    "print(f\"Average Tokens: {workflow_df['Tokens'].mean()}\")\n",
    "\n",
    "# Show accuracy results\n",
    "print(\"\\nMath Accuracy Results:\")\n",
    "display(accuracy_df)\n",
    "\n",
    "# Show workflow results\n",
    "print(\"\\nWorkflow Summary:\")\n",
    "display(workflow_df)\n",
    "\n",
    "# Count tool usage\n",
    "tools_used = {}\n",
    "for tools in workflow_df['Tools']:\n",
    "    for tool in tools.split(', '):\n",
    "        if tool:  # Skip empty strings\n",
    "            tools_used[tool] = tools_used.get(tool, 0) + 1\n",
    "\n",
    "print(\"\\nTools Used:\")\n",
    "display(pd.DataFrame(list(tools_used.items()), columns=['Tool', 'Count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Observability\n",
    "\n",
    "Now that we can run our agent as a service, we need to monitor its performance and behavior. AgentIQ provides comprehensive observability features that help us understand what's happening inside our agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observability and Profiling in AgentIQ\n",
    "\n",
    "AgentIQ offers comprehensive observability and profiling capabilities to monitor and optimize your workflows:\n",
    "\n",
    "1. **Telemetry Options**:\n",
    "   - **Logging**: Configure logs to console or file with different verbosity levels\n",
    "   - **Tracing**: Track the flow of requests through your system\n",
    "   - **Metrics**: Measure performance characteristics of your workflow\n",
    "\n",
    "2. **Profiling Tools**:\n",
    "   - **Token Usage Analysis**: Track and forecast token consumption\n",
    "   - **Latency Analysis**: Identify performance bottlenecks\n",
    "   - **Concurrency Analysis**: Understand parallel execution patterns\n",
    "\n",
    "3. **Tracing Providers**:\n",
    "   - **Phoenix Profiler**: A visualization tool by Arize AI for tracing and profiling\n",
    "   - **OpenTelemetry Collector**: Standard collector for observability data\n",
    "   - **Custom Providers**: Extensible system for custom telemetry exporters\n",
    "\n",
    "In this notebook, we'll use the **Phoenix Profiler** to visualize the execution of our agent and understand its performance characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Understanding Observability in AgentIQ\n",
    "\n",
    "AgentIQ supports multiple observability options, including:\n",
    "\n",
    "- **Logging Providers**: Console logging and file-based logging with configurable verbosity levels\n",
    "- **Tracing Providers**: Phoenix Profiler, OpenTelemetry Collector, and custom providers\n",
    "- **Metrics Collection**: Performance measurements for optimization\n",
    "\n",
    "For this notebook, we'll use the **Phoenix Profiler** for tracing. Phoenix is developed by Arize AI (https://github.com/Arize-ai/phoenix) and provides detailed insights into your agent's execution, including:\n",
    "\n",
    "- Visual representation of the agent's reasoning process\n",
    "- Timing information for each step and tool call\n",
    "- Token usage statistics and bottleneck identification\n",
    "- Hierarchical view of nested function calls\n",
    "\n",
    "These features help with debugging, performance optimization, and understanding usage patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Updating Configuration for Observability\n",
    "\n",
    "Let's update our configuration file to enable observability features. We'll add a `telemetry` section to the `general` configuration that includes logging and tracing settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/math_tools/configs/observability_config.yml\n",
    "\n",
    "general:\n",
    "  use_uvloop: true\n",
    "  telemetry:\n",
    "    logging:\n",
    "        console:\n",
    "            _type: console\n",
    "            level: WARN\n",
    "    tracing:\n",
    "        phoenix:\n",
    "            _type: phoenix\n",
    "            endpoint: http://phoenix:6006/v1/traces\n",
    "            project: math_tools_example\n",
    "\n",
    "functions:\n",
    "  calculator_exponent:\n",
    "    _type: calculator_exponent\n",
    "  calculator_modulus:\n",
    "    _type: calculator_modulus\n",
    "  calculator_square_root:\n",
    "    _type: calculator_square_root\n",
    "  calculator_add:\n",
    "    _type: calculator_add\n",
    "  current_datetime:\n",
    "    _type: current_datetime\n",
    "\n",
    "llms:\n",
    "  nim_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-70b-instruct\n",
    "    temperature: 0.0\n",
    "\n",
    "workflow:\n",
    "  _type: react_agent\n",
    "  tool_names:\n",
    "    - calculator_exponent\n",
    "    - calculator_modulus\n",
    "    - calculator_square_root\n",
    "    - calculator_add\n",
    "    - current_datetime\n",
    "  llm_name: nim_llm\n",
    "  verbose: true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Opening Phoenix\n",
    "\n",
    "Now we can open the Phoenix UI and see the profiling data. Feel free to run the query in the next cell multiple times (or change the query and run it) to see observability data flowing into Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "const href = window.location.hostname;\n",
    "let a = document.createElement('a');\n",
    "let link = document.createTextNode('Click here to open Phoenix!');\n",
    "a.appendChild(link);\n",
    "a.href = \"http://\" + href + \"/phoenix\";\n",
    "a.style.color = \"navy\"\n",
    "a.target = \"_blank\"\n",
    "element.append(a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Running AgentIQ with Observability Enabled\n",
    "\n",
    "Now let's run our agent with observability enabled. This will generate logs and traces that we can use to monitor and debug our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aiq run --config_file workflows/math_tools/configs/observability_config.yml --input \"What is the square root of the current hour plus 5?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have run the above (feel free to modify the query), look at Phoenix and you should see observability data flowing in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored how to evaluate and profile our AgentIQ workflow. We've learned how to:\n",
    "\n",
    "1. Create comprehensive evaluation datasets with test cases for different capabilities\n",
    "2. Configure and run evaluations using RAGAS and Trajectory evaluators\n",
    "3. Analyze evaluation results to understand agent performance\n",
    "4. Handle time-based queries and dynamic responses in evaluations\n",
    "5. Set up observability features using Phoenix Profiler by Arize AI for monitoring and debugging\n",
    "\n",
    "These techniques help ensure that our agent performs reliably and efficiently and provide insights for further improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
